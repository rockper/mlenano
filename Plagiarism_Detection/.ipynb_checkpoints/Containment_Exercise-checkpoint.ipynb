{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Containment\n",
    "\n",
    "In this notebook, you'll implement a containment function that looks at a source and answer text and returns a *normalized* value that represents the similarity between those two texts based on their n-gram intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# https://github.com/swastiknath/pyt_SM_plg_det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram counts\n",
    "\n",
    "One of the first things you'll need to do is to count up the occurrences of n-grams in your text data. To convert a set of text data into a matrix of counts, you can use a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Below, you can set a value for n and use a CountVectorizer is used to count up the n-gram occurrences. In the next cell, we'll see that the CountVectorizer constructs a vocabulary, and later, we'll look at the matrix of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text = 'pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  '\n",
    "s_text = 'pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pagerank': 131, 'is': 93, 'link': 102, 'analysis': 14, 'algorithm': 8, 'used': 198, 'by': 31, 'the': 185, 'google': 65, 'internet': 90, 'search': 168, 'engine': 53, 'that': 184, 'assigns': 23, 'numerical': 122, 'weighting': 212, 'to': 190, 'each': 51, 'element': 52, 'of': 124, 'hyperlinked': 75, 'set': 169, 'documents': 50, 'such': 180, 'as': 20, 'world': 219, 'wide': 215, 'web': 208, 'with': 216, 'purpose': 148, 'measuring': 111, 'its': 95, 'relative': 158, 'importance': 80, 'within': 217, 'numeric': 121, 'from': 63, '10': 0, 'for': 62, 'webpage': 209, 'on': 125, 'this': 189, 'denotes': 44, 'site': 173, 'in': 82, 'eyes': 59, 'derived': 46, 'theoretical': 187, 'probability': 141, 'value': 200, 'logarithmic': 105, 'scale': 167, 'like': 101, 'richter': 164, 'particular': 135, 'page': 130, 'roughly': 166, 'based': 26, 'upon': 196, 'quantity': 149, 'inbound': 83, 'links': 104, 'well': 213, 'pages': 132, 'providing': 146, 'may': 110, 'be': 27, 'applied': 18, 'any': 17, 'collection': 36, 'entities': 54, 'reciprocal': 155, 'quotations': 150, 'and': 16, 'references': 157, 'weight': 211, 'it': 94, 'given': 64, 'also': 11, 'called': 32, 'denoted': 43, 'pr': 138, 'known': 99, 'other': 129, 'factors': 60, 'relevance': 159, 'words': 218, 'actual': 7, 'visits': 202, 'reported': 161, 'toolbar': 191, 'influence': 89, 'ranking': 152, 'algorithms': 9, 'include': 84, 'hits': 71, 'invented': 92, 'jon': 97, 'kleinberg': 98, 'teoma': 182, 'now': 119, 'ask': 21, 'com': 37, 'ibm': 76, 'clever': 35, 'project': 143, 'trustrank': 193, 'name': 115, 'trademark': 192, 'process': 142, 'has': 66, 'been': 28, 'patented': 137, 'patent': 136, '285': 2, '999': 4, 'however': 73, 'assigned': 22, 'stanford': 178, 'university': 195, 'not': 118, 'exclusive': 57, 'license': 100, 'rights': 165, 'received': 153, 'million': 113, 'shares': 170, 'exchange': 56, 'use': 197, 'were': 214, 'sold': 174, '2005': 1, '336': 3, 'describes': 47, 'relies': 160, 'uniquely': 194, 'democratic': 42, 'nature': 116, 'using': 199, 'vast': 201, 'structure': 179, 'an': 13, 'indicator': 86, 'individual': 87, 'essence': 55, 'interprets': 91, 'vote': 204, 'but': 30, 'looks': 106, 'at': 24, 'more': 114, 'than': 183, 'sheer': 171, 'volume': 203, 'votes': 205, 'or': 126, 'receives': 154, 'analyzes': 15, 'casts': 34, 'cast': 33, 'are': 19, 'themselves': 186, 'important': 81, 'weigh': 210, 'heavily': 68, 'help': 69, 'make': 107, 'results': 163, 'ballot': 25, 'among': 12, 'all': 10, 'about': 5, 'how': 72, 'hyperlink': 74, 'counts': 40, 'support': 181, 'defined': 41, 'recursively': 156, 'depends': 45, 'number': 120, 'metric': 112, 'incoming': 85, 'linked': 103, 'many': 109, 'high': 70, 'rank': 151, 'itself': 96, 'if': 78, 'there': 188, 'no': 117, 'order': 127, 'prevent': 140, 'manipulation': 108, 'spoofing': 177, 'spamdexing': 175, 'provides': 145, 'specific': 176, 'details': 48, 'numerous': 123, 'academic': 6, 'papers': 134, 'concerning': 39, 'have': 67, 'published': 147, 'since': 172, 'brin': 29, 'original': 128, 'paper': 133, 'practice': 139, 'concept': 38, 'proven': 144, 'vulnerable': 206, 'extensive': 58, 'research': 162, 'devoted': 49, 'identifying': 77, 'falsely': 61, 'inflated': 88, 'ways': 207, 'ignore': 79}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# a_text = \"This is an answer text\"\n",
    "# s_text = \"This is a source text\"\n",
    "\n",
    "# set n\n",
    "n = 1\n",
    "\n",
    "# instantiate an ngram counter\n",
    "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "\n",
    "# create a dictionary of n-grams by calling `.fit`\n",
    "vocab2int = counts.fit([a_text, s_text]).vocabulary_\n",
    "\n",
    "# print dictionary of words:index\n",
    "print(vocab2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Create a vocabulary for 2-grams (aka \"bigrams\")\n",
    "\n",
    "Create a `CountVectorizer`, `counts_2grams`, and fit it to our text data. Print out the resultant vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this is': 5, 'is an': 2, 'an answer': 0, 'answer text': 1, 'is source': 3, 'source text': 4}\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary for 2-grams\n",
    "counts_2grams = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "vocab2int_2grams = counts_2grams.fit([a_text, s_text]).vocabulary_\n",
    "\n",
    "# print dictionary of words:index\n",
    "print(vocab2int_2grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes up a word?\n",
    "\n",
    "You'll note that the word \"a\" does not appear in the vocabulary. And also that the words have been converted to lowercase. When `CountVectorizer` is passed `analyzer='word'` it defines a word as *two or more* characters and so it ignores uni-character words. In a lot of text analysis, single characters are often irrelevant to the meaning of a passage, so leaving them out of a vocabulary is often desired behavior. \n",
    "\n",
    "For our purposes, this default behavior will work well; we don't need uni-character words to determine cases of plagiarism, but you may still want to experiment with uni-character counts.\n",
    "\n",
    "> If you *do* want to include single characters as words, you can choose to do so by adding one more argument when creating the `CountVectorizer`; pass in the definition of a token, `token_pattern = r\"(?u)\\b\\w+\\b\"`. \n",
    "\n",
    "This regular expression defines a word as one or more characters. If you want to learn more about this vectorizer, I suggest reading through the [source code](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L664), which is well documented.\n",
    "\n",
    "**Next, let's fit our `CountVectorizer` to all of our text data to make an array of n-gram counts!**\n",
    "\n",
    "The below code, assumes that `counts` is our `CountVectorizer` for the n-gram size we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0  0  0  0  0  1  4  1  0  2  0  0  1  0  5  2  1  0  3  1  0  3\n",
      "   0  0  2  1  0  0  0  5  1  0  0  1  1  1  0  0  0  0  0  1  1  0  1  0\n",
      "   0  0  1  2  2  1  1  0  0  0  0  1  1  0  2  2  1  4  0  0  0  0  0  1\n",
      "   0  0  0  1  1  0  0  0  2  0  1  1  1  0  0  0  0  1  2  0  1  5  2  1\n",
      "   0  1  1  1  0  1  2  0  2  1  0  0  0  0  1  1  0  0  0  0  0  0  0  1\n",
      "   0  1  2  0 10  3  0  0  0  2  3  7  2  0  0  1  0  0  1  0  0  1  0  1\n",
      "   0  0  1  0  1  1  1  0  1  0  0  1  0  1  1  1  0  1  0  0  1  0  1  2\n",
      "   2  2  0  0  0  1  0  0  0  0  0  0  1  0  1  0  3 23  0  1  0  1  4  1\n",
      "   0  1  0  0  1  0  2  0  1  0  1  0  0  0  0  0  2  1  0  1  2  1  0  1\n",
      "   2  1  1  1]\n",
      " [ 1  1  1  1  1  2  1  1  4  1  2  3  1  2  1  1 14  2  1  2  6  1  1  3\n",
      "   1  1  2  2  3  1  1  9  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  2  2  2  1  1  1  1  1  1  1  2  2  6  6  1 12  4  1  1  1  2  1\n",
      "   2  1  1  1  1  1  1  1  2  3  8  1  1  1  1  1  2  2  2  1  1 11  4  2\n",
      "   1  1  1  1  1  1  5  1  6  1  1  1  2  1  1  1  1  2  2  1  1  3  1  1\n",
      "   1  1  2  1 18  7  1  1  1  6 17 20  7  1  1  1  4  1  1  1  1  1  1  1\n",
      "   1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  2\n",
      "   2  2  2  1  1  1  1  1  1  1  2  1  1  2  1  1  8 40  1  1  2  1 17  1\n",
      "   1  1  1  3  1  1  2  1  2  1  1  1  3  2  1  1  5  1  1  1  2  1  1  2\n",
      "   4  1  2  2]]\n"
     ]
    }
   ],
   "source": [
    "# create array of n-gram counts for the answer and source text\n",
    "ngrams = counts.fit_transform([a_text, s_text])\n",
    "\n",
    "# row = the 2 texts and column = indexed vocab terms (as mapped above)\n",
    "# ex. column 0 = 'an', col 1 = 'answer'.. col 4 = 'text'\n",
    "ngram_array = ngrams.toarray()\n",
    "print(ngram_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num = len(set(np.where(ngram_array[0] > 0)[0]).intersection(set(np.where(ngram_array[1] > 0)[0])))\n",
    "# den = len(np.where(ngram_array[0] > 0)[0])\n",
    "num/den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the top row indicates the n-gram counts for the answer text `a_text`, and the second row indicates those for the source text `s_text`. If they have n-grams in common, you can see this by looking at the column values. For example they both have one \"is\" (column 2) and \"text\" (column 4) and \"this\" (column 5).\n",
    "\n",
    "```\n",
    "[[1 1 1 0 1 1]    =   an  answer  [is]  ______  [text] [this]\n",
    " [0 0 1 1 1 1]]   =   __  ______  [is]  source  [text] [this]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Calculate containment values\n",
    "\n",
    "Assume your function takes in an `ngram_array` just like that generated above, for an answer text (row 0) and a source text (row 1). Using just this information, calculate the containment between the two texts. As before, it's okay to ignore the uni-character words.\n",
    "\n",
    "To calculate the containment:\n",
    "1. Calculate the n-gram **intersection** between the answer and source text.\n",
    "2. Add up the number of common terms.\n",
    "3. Normalize by dividing the value in step 2 by the number of n-grams in the answer text.\n",
    "\n",
    "The complete equation is:\n",
    "\n",
    "$$ \\frac{\\sum{count(\\text{ngram}_{A}) \\cap count(\\text{ngram}_{S})}}{\\sum{count(\\text{ngram}_{A})}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containment(ngram_array):\n",
    "    ''' Containment is a measure of text similarity. It is the normalized, \n",
    "       intersection of ngram word counts in two texts.\n",
    "       :param ngram_array: an array of ngram counts for an answer and source text.\n",
    "       :return: a normalized containment value.'''\n",
    "    \n",
    "    \n",
    "    # your code here\n",
    "    numerator = np.sum(np.amin(ngram_array, axis=0))\n",
    "    denominator = np.sum(ngram_array[0])\n",
    "    cont = numerator / denominator\n",
    "    return cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containment:  0.6\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# test out your code\n",
    "containment_val = containment(ngrams.toarray())\n",
    "\n",
    "print('Containment: ', containment_val)\n",
    "\n",
    "# note that for the given texts, and n = 1\n",
    "# the containment value should be 3/5 or 0.6\n",
    "assert containment_val==0.6, 'Unexpected containment value for n=1.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for n = 2\n",
    "counts_2grams = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "bigram_counts = counts_2grams.fit_transform([a_text, s_text])\n",
    "\n",
    "# calculate containment\n",
    "containment_val = containment(bigram_counts.toarray())\n",
    "\n",
    "print('Containment for n=2 : ', containment_val)\n",
    "\n",
    "# the containment value should be 1/4 or 0.25\n",
    "assert containment_val==0.25, 'Unexpected containment value for n=2.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend trying out different phrases, and different values of n. What happens if you count for uni-character words? What if you make the sentences much larger?\n",
    "\n",
    "I find that the best way to understand a new concept is to think about how it might be applied in a variety of different ways."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
